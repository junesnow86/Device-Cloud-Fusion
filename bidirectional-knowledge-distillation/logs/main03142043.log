Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data/cifar-100-python.tar.gz
Extracting ./data/cifar-100-python.tar.gz to ./data
Files already downloaded and verified
>>> Start training cloud model...
Files already downloaded and verified
Files already downloaded and verified
>>> Start training cloud model...
Epoch 1: training loss=4.6243815119304355, validation loss=4.598526448011398
Epoch 2: training loss=4.567072005498977, validation loss=4.601482659578323
Epoch 3: training loss=4.5444964984106635, validation loss=4.572295784950256
Epoch 4: training loss=4.50606430144537, validation loss=4.556032210588455
Epoch 5: training loss=4.465102089775933, validation loss=4.527462422847748
Epoch 6: training loss=4.433193721468487, validation loss=4.5008008778095245
Epoch 7: training loss=4.385664841485402, validation loss=4.481314301490784
Epoch 8: training loss=4.341863738165961, validation loss=4.458638399839401
Epoch 9: training loss=4.29377770045447, validation loss=4.4476205706596375
Epoch 10: training loss=4.252006424797906, validation loss=4.424780249595642
Epoch 11: training loss=4.223649176340254, validation loss=4.379459589719772
Epoch 12: training loss=4.181436262433491, validation loss=4.379937708377838
Epoch 13: training loss=4.152187892368862, validation loss=4.356540739536285
Epoch 14: training loss=4.106616739242796, validation loss=4.345903128385544
Epoch 15: training loss=4.078659227916172, validation loss=4.311238586902618
Epoch 16: training loss=4.029276677540371, validation loss=4.313269674777985
Epoch 17: training loss=3.975319714773269, validation loss=4.306909292936325
Epoch 18: training loss=3.9502727720472546, validation loss=4.285067826509476
Epoch 19: training loss=3.8979431144774908, validation loss=4.289255440235138
Epoch 20: training loss=3.88259027874659, validation loss=4.296192318201065
Epoch 21: training loss=3.8470402747865706, validation loss=4.277533143758774
Epoch 22: training loss=3.771613003715636, validation loss=4.278940498828888
Epoch 23: training loss=3.7381843271709623, validation loss=4.285603016614914
Epoch 24: training loss=3.692698599800231, validation loss=4.285179525613785
Early stop at epoch 24.
>>> Start training party A model...
Epoch 1: training loss=4.588334029255389, validation loss=4.5707562137657485
Epoch 2: training loss=4.549471855163574, validation loss=4.525596786552752
Epoch 3: training loss=4.4912365288921094, validation loss=4.458448624946702
Epoch 4: training loss=4.414797984832546, validation loss=4.368075955081993
Epoch 5: training loss=4.298588062096321, validation loss=4.230591162829332
Epoch 6: training loss=4.148239800090043, validation loss=4.06985501840081
Epoch 7: training loss=3.9659223743180787, validation loss=3.878933997221396
Epoch 8: training loss=3.7578571716661555, validation loss=3.6833954226802774
Epoch 9: training loss=3.603375513782705, validation loss=3.5804961029912383
Epoch 10: training loss=3.5125358257429458, validation loss=3.5085569536182244
Epoch 11: training loss=3.4413638157352433, validation loss=3.4640834801633593
Epoch 12: training loss=3.387014677516082, validation loss=3.423919953091044
Epoch 13: training loss=3.336960084073484, validation loss=3.3819854225910886
Epoch 14: training loss=3.2974306272866465, validation loss=3.3625314134947013
Epoch 15: training loss=3.2434483108995646, validation loss=3.3213698259541684
Epoch 16: training loss=3.1973712020073073, validation loss=3.2984025075402057
Epoch 17: training loss=3.1613684693265216, validation loss=3.2690794736566677
Epoch 18: training loss=3.1176895238326536, validation loss=3.254207540565813
Epoch 19: training loss=3.0792241011650114, validation loss=3.2373152450776437
Epoch 20: training loss=3.032752111713233, validation loss=3.2060008955673434
Epoch 21: training loss=2.9973935173075392, validation loss=3.1676630940235837
Epoch 22: training loss=2.9539862239063845, validation loss=3.162233117600562
Epoch 23: training loss=2.909073629413211, validation loss=3.1466236853263747
Epoch 24: training loss=2.882758756549333, validation loss=3.1471484983471076
Epoch 25: training loss=2.853975833098659, validation loss=3.1195839962489167
Epoch 26: training loss=2.821308985299487, validation loss=3.115588967229279
Epoch 27: training loss=2.7879935142408487, validation loss=3.149189401680315
Epoch 28: training loss=2.7562318408192263, validation loss=3.108849300465114
Epoch 29: training loss=2.7288458330351264, validation loss=3.111088746030566
Epoch 30: training loss=2.691451798130185, validation loss=3.1155813311187313
Epoch 31: training loss=2.6575011748860313, validation loss=3.0963772955075117
Epoch 32: training loss=2.6422061470479727, validation loss=3.125304299341121
Epoch 33: training loss=2.6019205020415823, validation loss=3.1334651691812865
Epoch 34: training loss=2.5780576734780416, validation loss=3.1022207904869403
Early stop at epoch 34.
>>> Start training party B model...
Epoch 1: training loss=4.563613938947096, validation loss=4.478136740939718
Epoch 2: training loss=4.383313926399177, validation loss=4.328564274478966
Epoch 3: training loss=4.248401344245207, validation loss=4.197291038405727
Epoch 4: training loss=4.0885247761476124, validation loss=3.979682697376735
Epoch 5: training loss=3.8108181818157223, validation loss=3.713929599439594
Epoch 6: training loss=3.639318078967696, validation loss=3.593550440291284
Epoch 7: training loss=3.5425416357973787, validation loss=3.5147887552288215
Epoch 8: training loss=3.449294777626687, validation loss=3.438233986706801
Epoch 9: training loss=3.380768824130931, validation loss=3.392150731153891
Epoch 10: training loss=3.3181818546132837, validation loss=3.3409306029198875
Epoch 11: training loss=3.2688742148960737, validation loss=3.295933941720237
Epoch 12: training loss=3.222834824670291, validation loss=3.2597282805912933
Epoch 13: training loss=3.179503917694092, validation loss=3.229211625918536
Epoch 14: training loss=3.126222237627557, validation loss=3.205186602095483
Epoch 15: training loss=3.0743037352325224, validation loss=3.143362713531709
Epoch 16: training loss=3.0001266653656113, validation loss=3.0925859028184917
Epoch 17: training loss=2.927848625690379, validation loss=3.0340295375232964
Epoch 18: training loss=2.864752157360104, validation loss=2.988058734947527
Epoch 19: training loss=2.8076052885529, validation loss=2.969597322840086
Epoch 20: training loss=2.752044123960725, validation loss=2.9357101749366437
Epoch 21: training loss=2.6882924670023276, validation loss=2.898772206104977
Epoch 22: training loss=2.652615343425291, validation loss=2.8580672472295627
Epoch 23: training loss=2.603416830089921, validation loss=2.848445146856174
Epoch 24: training loss=2.5454989828116505, validation loss=2.847527181598502
Epoch 25: training loss=2.4980712283587625, validation loss=2.81350770802565
Epoch 26: training loss=2.4522544551402965, validation loss=2.812615995675745
Epoch 27: training loss=2.425965091860886, validation loss=2.794563246444917
Epoch 28: training loss=2.3678090411720545, validation loss=2.776575887706918
Epoch 29: training loss=2.3293673586338124, validation loss=2.7881516503616117
Epoch 30: training loss=2.2852727842669114, validation loss=2.783171465699102
Epoch 31: training loss=2.242549588494267, validation loss=2.7690653129362723
Epoch 32: training loss=2.1852702667527164, validation loss=2.788266272612021
Epoch 33: training loss=2.154862921711401, validation loss=2.7706233850667172
Epoch 34: training loss=2.1260407300705606, validation loss=2.8031530581729514
Early stop at epoch 34.
>>> Start distillation from party A to cloud...
Epoch 1: training loss=0.5676999507849751
Epoch 2: training loss=0.4884489764098171
Epoch 3: training loss=0.4448835743703876
Epoch 4: training loss=0.41316326376170026
Epoch 5: training loss=0.3924701163989369
>>> Start distillation from party B to cloud...
Epoch 1: training loss=0.9184617004918714
Epoch 2: training loss=0.9015041908896562
Epoch 3: training loss=0.8668126863368014
Epoch 4: training loss=0.8260547696275914
Epoch 5: training loss=0.8128406980358962
>>> Start distillation from cloud to party A...
Epoch 1: training loss=1.6530544379400829
Epoch 2: training loss=1.4106031750875807
Epoch 3: training loss=1.7165367537074618
Epoch 4: training loss=1.5754488091620187
Epoch 5: training loss=1.4462551871935527
>>> Start distillation from cloud to party B...
Epoch 1: training loss=1.5481342115099468
Epoch 2: training loss=1.4521667484253171
Epoch 3: training loss=1.5565237819202362
Epoch 4: training loss=1.4575268749206785
Epoch 5: training loss=1.3288066689930265
Cloud model accuracy: 0.0660
Party A model accuracy: 0.1164
Party B model accuracy: 0.1393
Cloud model accuracy after distillation from party A: 0.0063
Cloud model accuracy after distillation from party B: 0.0219
Party A accuracy after distillation from cloud: 0.0451
Party B accuracy after distillation from cloud: 0.0574

Comments:
1. 使用了CIFAR-100数据集，但是来回蒸馏还是导致端侧模型性能下降
2. 猜测原因一方面是特征提取模块和分类器存在不一致，另一方面是知识蒸馏这个过程本身也不是很理想，知识蒸馏过程也依赖于使用的数据
